{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de8945bf",
   "metadata": {},
   "source": [
    "## mpi4py example (https://www.youtube.com/watch?v=36nCgG40DJo&t=1215s):\n",
    "allreduce functions listed: https://education.molssi.org/parallel-programming/03-distributed-examples-mpi4py/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "865064c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpi4py import MPI\n",
    "comm = MPI.COMM_WORLD\n",
    "my_rank = comm.Get_rank()\n",
    "print(\"my_rank =\",my_rank)\n",
    "p = comm.Get_size()\n",
    "print(\"p =\",p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040be109",
   "metadata": {},
   "outputs": [],
   "source": [
    "if my_rank != 0:\n",
    "    message = \"Hello from \"+str(my_rank)\n",
    "    comm.send(message, dest=0)\n",
    "else:\n",
    "    for procid in range(1,p):\n",
    "        message = comm.recv(source=procid)\n",
    "        print(\"process 0 receives message from process\", procid,\":\",message)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79224a93",
   "metadata": {},
   "source": [
    "#### With all reduce:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b89a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if my_rank == 0:\n",
    "    data = [1,2,3]\n",
    "    all_data_sum = comm.allreduce(data, op=MPI.SUM)\n",
    "    print(\"process \",my_rank,\"has computed sum =\", all_data_sum)\n",
    "else:\n",
    "    data = [4,5,6]\n",
    "    all_data_sum = comm.allreduce(data, op=MPI.SUM)\n",
    "    print(\"process \",my_rank,\"has computed sum =\", all_data_sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f15dcf",
   "metadata": {},
   "source": [
    "Call with:\n",
    "mpiexec -n 4 python script.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce8c742c",
   "metadata": {},
   "source": [
    "## MapReduce example (https://www.youtube.com/watch?v=EmHc9hV5Xi8):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71224615",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3263b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RatingsBreakdown(MRJob):\n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper=self.mapper_get_ratings,\n",
    "                  reducer=self.reducer_count_ratings)\n",
    "        ]\n",
    "    \n",
    "    def mapper_get_ratings(self, _, line):\n",
    "        (userID, movieID, rating, timestamp) = line.split('\\t')\n",
    "        return rating, 1\n",
    "    \n",
    "    def reducer_count_ratings(self, key, values):\n",
    "        return key, sum(values)\n",
    "    \n",
    "#if __name__ = '__main__':\n",
    "RatingsBreakdown.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a190bf1",
   "metadata": {},
   "source": [
    "## Multiprocessing async SGD for training a linear regression model example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ce4b49ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse\n",
    "import numpy as np\n",
    "from multiprocessing.sharedctypes import Array\n",
    "from ctypes import c_double\n",
    "from multiprocessing import Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ef0d8d18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size training data: (20000, 10)\n",
      "Size training labels: (20000, 1)\n"
     ]
    }
   ],
   "source": [
    "n=10 # number of features\n",
    "m=20000 # number of training examples\n",
    "\n",
    "X = scipy.sparse.random(m,n, density=.2).toarray() # Guarantees sparse grad updates\n",
    "real_w = np.random.uniform(0,1,size=(n,1)) # Define our true weight vector\n",
    "\n",
    "X = X/X.max() # Normalizing for training\n",
    "\n",
    "y = np.dot(X,real_w)\n",
    "\n",
    "print(\"Size training data:\", X.shape)\n",
    "print(\"Size training labels:\", y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037f92eb",
   "metadata": {},
   "source": [
    "First, we need to define the weight vector w in shared memory that can be accessed without locks. We will have to use the “sharedctypes.Array” class from multiprocessing for this functionality. We will further use numpy’s frombuffer function to make it accessible from a numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3a0d009c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size accessible w: (10, 1)\n"
     ]
    }
   ],
   "source": [
    "coef_shared = Array(c_double, \n",
    "        (np.random.normal(size=(n,1)) * 1./np.sqrt(n)).flat,\n",
    "        lock=False) # Hogwild!\n",
    "w = np.frombuffer(coef_shared)\n",
    "w = w.reshape((n,1)) \n",
    "print(\"size accessible w:\", w.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cab67f7",
   "metadata": {},
   "source": [
    "Our ultimate goal is to perform the gradient update in parallel. To do this, we need to define what this update is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1a3fd2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The calculation has been adjusted to allow for mini-batches\n",
    "learning_rate = .001\n",
    "def mse_gradient_step(X_y_tuple):\n",
    "    global w # Only for instructive purposes!\n",
    "    X, y = X_y_tuple # Required for how multiprocessing.Pool.map works\n",
    "    \n",
    "    # Calculate the gradient\n",
    "    err = y.reshape((len(y),1))-np.dot(X,w)\n",
    "    grad = -2.*np.dot(np.transpose(X),err)/ X.shape[0]\n",
    "\n",
    "    # Update the nonzero weights one at a time\n",
    "    for index in np.where(abs(grad) > .01)[0]:\n",
    "        coef_shared[index] -= learning_rate*grad[index,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16c79d5",
   "metadata": {},
   "source": [
    "Preparing the examples for multiprocessing.Pool: We are going to have to cut the training examples into tuples, one per example, to pass to our workers. We will also be reshaping them to adhere to the way the gradient step is written above. Code is included to allow you to use mini-batches via the batch_size variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8583f64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=1\n",
    "examples=[None]*int(X.shape[0]/float(batch_size))\n",
    "for k in range(int(X.shape[0]/float(batch_size))):\n",
    "    Xx = X[k*batch_size : (k+1)*batch_size,:].reshape((batch_size,X.shape[1]))\n",
    "    yy = y[k*batch_size : (k+1)*batch_size].reshape((batch_size,1))\n",
    "    examples[k] = (Xx, yy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd43c90",
   "metadata": {},
   "source": [
    "The Asynchronous Bit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ddece5",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = Pool(5)  \n",
    "p.map(mse_gradient_step, examples)\n",
    "\n",
    "print('Loss function on the training set:', np.mean(abs(y-np.dot(X,w))))\n",
    "print('Difference from the real weight vector:', abs(real_w-w).sum())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
